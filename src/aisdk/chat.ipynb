{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a copilot using Azure AI Studio\n",
    "\n",
    "In this tutorial, you will learn how to build, run, and evaluate, a copilot application using your own custom data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1 | Pre-Work: Setup AI Project**\n",
    "\n",
    "Before opening this Notebook, we assume you already completed the following steps:\n",
    "\n",
    "1. [Step 1](./../docs/step-01.md) - setup your development environment.\n",
    "1. [Step 2](./../docs/step-02.md) - run `ai init` to create AI project\n",
    "1. [Step 3a](./../docs/step-03.md) - run `ai search` to create Search index\n",
    "1. [Step 3b](./../docs/step-03.md) - run `ai dev new .env` to save env vars.\n",
    "\n",
    "ðŸŽ‰ _Congratulations!_ - You are now ready to start working with the Azure AI Studio SDK to interact with your Azure AI resources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 | Setup: Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (from .env)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 | Define: Azure AI Search Helper Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Function 1 => GET INDEXED DOCUMENTS FOR QUERY\n",
    "## Given a query (question)and a desired number of documents (default=5)\n",
    "## Retrieve the relevant set of matching results for the query from Cognitive Search\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import RawVectorQuery\n",
    "\n",
    "async def get_documents(query, num_docs=5):\n",
    "\n",
    "    #  retrieve documents relevant to the user's question from Cognitive Search\n",
    "    search_client = SearchClient(\n",
    "        endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"],\n",
    "        credential=AzureKeyCredential(os.environ[\"AZURE_AI_SEARCH_KEY\"]),\n",
    "        index_name=os.environ[\"AZURE_AI_SEARCH_INDEX_NAME\"])\n",
    "\n",
    "    # generate a vector embedding of the user's question\n",
    "    embedding = await openai.Embedding.acreate(input=query,\n",
    "                                               model=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "                                               deployment_id=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"])\n",
    "    embedding_to_query = embedding[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    # initialize context, then update it with results from query\n",
    "    context = \"\"\n",
    "    async with search_client:\n",
    "\n",
    "        # use the vector embedding to do a vector search on the index\n",
    "        vector_query = RawVectorQuery(vector=embedding_to_query, k=num_docs, fields=\"contentVector\")\n",
    "        results = await search_client.search(\n",
    "            search_text=\"\",\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"id\", \"content\"])\n",
    "\n",
    "        async for result in results:\n",
    "            context += f\"\\n>>> From: {result['id']}\\n{result['content']}\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 | Define Azure OpenAI ChatCompletion Helper Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Function 2 => GET CHAT COMPLETION MESSAGES\n",
    "## Given a list of user messages, a streaming flag, session state and context\n",
    "##    Call get_documents to get 5 relevant results matching the user question\n",
    "##    Make copy of context and modify to add retrieved documents\n",
    "##    Add retrieved documents to the system prompt section\n",
    "##    Call Azure OpenAI with the system prompt and user question\n",
    "\n",
    "import jinja2\n",
    "import pathlib\n",
    "\n",
    "from streaming_utils import add_context_to_streamed_response\n",
    "\n",
    "templateLoader = jinja2.FileSystemLoader(searchpath=\"./\")\n",
    "templateEnv = jinja2.Environment(loader=templateLoader)\n",
    "system_message_template = templateEnv.get_template(\"system-message.jinja2\")\n",
    "\n",
    "async def chat_completion(messages: list[dict], stream: bool = False,\n",
    "                          session_state: any = None, context: dict[str, any] = {}):\n",
    "    # get search documents for the last user message in the conversation\n",
    "    user_message = messages[-1][\"content\"]\n",
    "    documents = await get_documents(user_message, context.get(\"num_retrieved_docs\", 5))\n",
    "\n",
    "    # make a copy of the context and modify it with the retrieved documents\n",
    "    context = dict(context)\n",
    "    context['documents'] = documents\n",
    "\n",
    "    # add retrieved documents as context to the system prompt\n",
    "    system_message = system_message_template.render(context=context)\n",
    "    messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    # call Azure OpenAI with the system prompt and user's question\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),\n",
    "        messages=messages, temperature=context.get(\"temperature\", 0.7),\n",
    "        stream=stream,\n",
    "        max_tokens=800)\n",
    "\n",
    "    # add context in the returned response\n",
    "    if not stream:\n",
    "        response.choices[0]['context'] = context\n",
    "    else:\n",
    "        response = add_context_to_streamed_response(response, context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 | Invoke the Chat Completion Endpoint with the User Question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation = \"aisdk\"  (default only)\n",
    "# For now just invoke the method in the default python script.\n",
    "from chat import chat_completion\n",
    "\n",
    "# Simple question test\n",
    "question = \"Which tent is the most waterproof?\"\n",
    "\n",
    "# Define an async function to call the chat function\n",
    "result = await chat_completion([{\"role\": \"user\", \"content\": question}], stream=False)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
